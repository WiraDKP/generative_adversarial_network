{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2DegsYIgrHr"
   },
   "outputs": [],
   "source": [
    "!pip install jcopdl gdown\n",
    "!gdown https://drive.google.com/uc?id=1KaiwyyYRGW8FbvSd4Feg1i1YW2k2s30u\n",
    "!unzip /content/celebA_redux.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HGpzFWM3fG31",
    "outputId": "92a25837-f27f-4546-e9e9-8107b82b713e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from jcopdl.callback import Callback, set_config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KiSL1MrfG35"
   },
   "source": [
    "# Dataset & Dataloader (Hanya Train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-Zfd6T4fG35"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DA1TW9MvfG38"
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # menjadi (-1, 1)\n",
    "])\n",
    "\n",
    "train_set = datasets.ImageFolder(\"/content/celebA_redux\", transform=transform)\n",
    "trainloader = DataLoader(train_set, batch_size=bs, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jmUnJ5UfG3-"
   },
   "source": [
    "# Arsitektur & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MWXOXKz86WgO",
    "outputId": "cfbbbb02-6436-4b22-d6ac-0316f72612f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_wdcgan.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_wdcgan.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from jcopdl.layers import conv_block, tconv_block, linear_block\n",
    "\n",
    "def conv(c_in, c_out, batch_norm=True, activation=\"lrelu\"):\n",
    "    return conv_block(c_in, c_out, kernel=4, stride=2, pad=1, bias=False, batch_norm=batch_norm, activation=activation, pool_type=None)\n",
    "\n",
    "def tconv(c_in, c_out, batch_norm=True, activation=\"lrelu\"):\n",
    "    return tconv_block(c_in, c_out, kernel=4, stride=2, pad=1, bias=False, batch_norm=batch_norm, activation=activation, pool_type=None)  \n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            conv(3, 32, batch_norm=False),          \n",
    "            conv(32, 64),\n",
    "            conv(64, 128),\n",
    "            conv(128, 256),\n",
    "            conv_block(256, 1, kernel=4, stride=1, pad=0, bias=False, activation=None, pool_type=None),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    def clip_weights(self, vmin=-0.01, vmax=0.01):\n",
    "        for p in self.parameters():\n",
    "            p.data.clamp_(vmin, vmax)    \n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.tconv = nn.Sequential(\n",
    "            tconv_block(z_dim, 512, kernel=4, stride=2, pad=1, bias=False, activation=\"lrelu\", pool_type=None),\n",
    "            tconv(512, 256),\n",
    "            tconv(256, 128),\n",
    "            tconv(128, 64),\n",
    "            tconv(64, 32),\n",
    "            tconv(32, 3, activation=\"tanh\", batch_norm=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.tconv(x)\n",
    "\n",
    "    def generate(self, n, device):\n",
    "        z = torch.randn((n, self.z_dim, 1, 1), device=device)\n",
    "        return self.tconv(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbNdKhoFfG4G"
   },
   "outputs": [],
   "source": [
    "config = set_config({\n",
    "    \"z_dim\": 100,\n",
    "    \"batch_size\": bs\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUp8C9WLfG4I"
   },
   "source": [
    "# Training Preparation -> MCOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6UVaR9zCOu4"
   },
   "outputs": [],
   "source": [
    "from model_wdcgan import Critic, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0p5r_GaqL0_r"
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(output, target):\n",
    "    return output.mean() * target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tW1wM31CfG4J"
   },
   "outputs": [],
   "source": [
    "D = Critic().to(device)\n",
    "G = Generator(config.z_dim).to(device)\n",
    "\n",
    "criterion = wasserstein_loss\n",
    "\n",
    "d_optimizer = optim.RMSprop(D.parameters(), lr=1e-4)\n",
    "g_optimizer = optim.RMSprop(G.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGvNxhjifG4L"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsqZrS3E5TzE"
   },
   "outputs": [],
   "source": [
    "# !rm -rf /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vFrvrads2YK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.makedirs(\"output/WDCGAN/\", exist_ok=True)\n",
    "os.makedirs(\"model/WDCGAN/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7KP2mzSOpem2",
    "outputId": "22e2fca5-1e6c-49d6-e7ec-790403c10a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0 | D_loss: -0.18430 | G_loss: 0.18856\n",
      "Epoch:     5 | D_loss: -0.19710 | G_loss: 0.19879\n",
      "Epoch:    10 | D_loss: -0.19709 | G_loss: 0.19913\n",
      "Epoch:    15 | D_loss: -0.19839 | G_loss: 0.20005\n",
      "Epoch:    20 | D_loss: -0.19784 | G_loss: 0.19978\n",
      "Epoch:    25 | D_loss: -0.19721 | G_loss: 0.19912\n",
      "Epoch:    30 | D_loss: -0.19846 | G_loss: 0.20011\n",
      "Epoch:    35 | D_loss: -0.04034 | G_loss: 0.16971\n",
      "Epoch:    40 | D_loss: -0.19820 | G_loss: 0.20010\n",
      "Epoch:    45 | D_loss: -0.19777 | G_loss: 0.19922\n",
      "Epoch:    50 | D_loss: -0.19442 | G_loss: 0.19559\n",
      "Epoch:    55 | D_loss: -0.19797 | G_loss: 0.19981\n",
      "Epoch:    60 | D_loss: -0.19739 | G_loss: 0.19946\n",
      "Epoch:    65 | D_loss: -0.19824 | G_loss: 0.19982\n",
      "Epoch:    70 | D_loss: -0.19810 | G_loss: 0.19980\n",
      "Epoch:    75 | D_loss: -0.19585 | G_loss: 0.19823\n",
      "Epoch:    80 | D_loss: -0.18828 | G_loss: 0.18834\n",
      "Epoch:    85 | D_loss: -0.19728 | G_loss: 0.19964\n",
      "Epoch:    90 | D_loss: -0.01078 | G_loss: -0.19155\n",
      "Epoch:    95 | D_loss: -0.19719 | G_loss: 0.19912\n",
      "Epoch:   100 | D_loss: -0.19678 | G_loss: 0.19893\n",
      "Epoch:   105 | D_loss: -0.19590 | G_loss: 0.19766\n",
      "Epoch:   110 | D_loss: -0.19629 | G_loss: 0.19832\n",
      "Epoch:   115 | D_loss: -0.19666 | G_loss: 0.19843\n",
      "Epoch:   120 | D_loss: -0.19746 | G_loss: 0.19930\n",
      "Epoch:   125 | D_loss: -0.19776 | G_loss: 0.19943\n",
      "Epoch:   130 | D_loss: -0.19577 | G_loss: 0.19796\n",
      "Epoch:   135 | D_loss: -0.19703 | G_loss: 0.19868\n",
      "Epoch:   140 | D_loss: -0.19468 | G_loss: 0.19751\n",
      "Epoch:   145 | D_loss: -0.19671 | G_loss: 0.19841\n",
      "Epoch:   150 | D_loss: -0.00390 | G_loss: 0.19642\n",
      "Epoch:   155 | D_loss: -0.19769 | G_loss: 0.19951\n",
      "Epoch:   160 | D_loss: -0.19739 | G_loss: 0.19916\n",
      "Epoch:   165 | D_loss: -0.19384 | G_loss: 0.19670\n",
      "Epoch:   170 | D_loss: -0.19669 | G_loss: 0.19850\n",
      "Epoch:   175 | D_loss: -0.19531 | G_loss: 0.19789\n",
      "Epoch:   180 | D_loss: -0.19624 | G_loss: 0.19817\n",
      "Epoch:   185 | D_loss: -0.00905 | G_loss: 0.19470\n",
      "Epoch:   190 | D_loss: -0.19699 | G_loss: 0.19887\n",
      "Epoch:   195 | D_loss: -0.19605 | G_loss: 0.19837\n",
      "Epoch:   200 | D_loss: -0.19390 | G_loss: 0.19564\n",
      "Epoch:   205 | D_loss: -0.19245 | G_loss: 0.19461\n",
      "Epoch:   210 | D_loss: -0.19297 | G_loss: 0.19579\n",
      "Epoch:   215 | D_loss: -0.18973 | G_loss: 0.19374\n",
      "Epoch:   220 | D_loss: -0.19438 | G_loss: 0.19683\n",
      "Epoch:   225 | D_loss: -0.19385 | G_loss: 0.19673\n",
      "Epoch:   230 | D_loss: -0.19649 | G_loss: 0.19862\n",
      "Epoch:   235 | D_loss: -0.19258 | G_loss: 0.19547\n",
      "Epoch:   240 | D_loss: -0.19445 | G_loss: 0.19717\n",
      "Epoch:   245 | D_loss: -0.19301 | G_loss: 0.19703\n",
      "Epoch:   250 | D_loss: -0.18908 | G_loss: 0.19386\n",
      "Epoch:   255 | D_loss: -0.19376 | G_loss: 0.19688\n",
      "Epoch:   260 | D_loss: -0.19202 | G_loss: 0.19471\n",
      "Epoch:   265 | D_loss: -0.19377 | G_loss: 0.19663\n",
      "Epoch:   270 | D_loss: -0.18143 | G_loss: 0.18724\n",
      "Epoch:   275 | D_loss: -0.00811 | G_loss: -0.17556\n",
      "Epoch:   280 | D_loss: -0.19284 | G_loss: 0.19560\n",
      "Epoch:   285 | D_loss: -0.14095 | G_loss: 0.18578\n",
      "Epoch:   290 | D_loss: -0.19286 | G_loss: 0.19714\n",
      "Epoch:   295 | D_loss: -0.19344 | G_loss: 0.19677\n",
      "Epoch:   300 | D_loss: -0.02072 | G_loss: -0.15923\n",
      "Epoch:   305 | D_loss: -0.18983 | G_loss: 0.19549\n",
      "Epoch:   310 | D_loss: -0.18895 | G_loss: 0.19492\n",
      "Epoch:   315 | D_loss: -0.19252 | G_loss: 0.19460\n",
      "Epoch:   320 | D_loss: -0.18418 | G_loss: 0.18961\n",
      "Epoch:   325 | D_loss: -0.19072 | G_loss: 0.19477\n",
      "Epoch:   330 | D_loss: -0.19273 | G_loss: 0.19609\n",
      "Epoch:   335 | D_loss: -0.00328 | G_loss: 0.19331\n",
      "Epoch:   340 | D_loss: -0.19206 | G_loss: 0.19544\n",
      "Epoch:   345 | D_loss: -0.19151 | G_loss: 0.19564\n",
      "Epoch:   350 | D_loss: -0.19067 | G_loss: 0.19637\n",
      "Epoch:   355 | D_loss: -0.18994 | G_loss: 0.19478\n",
      "Epoch:   360 | D_loss: -0.19117 | G_loss: 0.19616\n",
      "Epoch:   365 | D_loss: -0.17153 | G_loss: 0.19314\n",
      "Epoch:   370 | D_loss: -0.19193 | G_loss: 0.19574\n",
      "Epoch:   375 | D_loss: -0.18468 | G_loss: 0.19078\n",
      "Epoch:   380 | D_loss: -0.05064 | G_loss: 0.17710\n",
      "Epoch:   385 | D_loss: -0.19080 | G_loss: 0.19498\n",
      "Epoch:   390 | D_loss: -0.19117 | G_loss: 0.19413\n",
      "Epoch:   395 | D_loss: -0.18719 | G_loss: 0.19196\n",
      "Epoch:   400 | D_loss: -0.19203 | G_loss: 0.19526\n",
      "Epoch:   405 | D_loss: -0.18734 | G_loss: 0.19126\n",
      "Epoch:   410 | D_loss: -0.19138 | G_loss: 0.19371\n",
      "Epoch:   415 | D_loss: -0.18973 | G_loss: 0.19414\n",
      "Epoch:   420 | D_loss: -0.19029 | G_loss: 0.19568\n",
      "Epoch:   425 | D_loss: -0.19217 | G_loss: 0.19683\n",
      "Epoch:   430 | D_loss: -0.18986 | G_loss: 0.19292\n",
      "Epoch:   435 | D_loss: -0.18524 | G_loss: 0.18724\n",
      "Epoch:   440 | D_loss: -0.18967 | G_loss: 0.19393\n",
      "Epoch:   445 | D_loss: -0.18985 | G_loss: 0.19443\n",
      "Epoch:   450 | D_loss: -0.18882 | G_loss: 0.19317\n",
      "Epoch:   455 | D_loss: -0.18590 | G_loss: 0.19134\n",
      "Epoch:   460 | D_loss: -0.19013 | G_loss: 0.19584\n",
      "Epoch:   465 | D_loss: -0.19016 | G_loss: 0.19277\n",
      "Epoch:   470 | D_loss: -0.19002 | G_loss: 0.19327\n",
      "Epoch:   475 | D_loss: -0.19108 | G_loss: 0.19702\n",
      "Epoch:   480 | D_loss: -0.18932 | G_loss: 0.19532\n",
      "Epoch:   485 | D_loss: -0.18694 | G_loss: 0.19295\n",
      "Epoch:   490 | D_loss: -0.18275 | G_loss: 0.19026\n",
      "Epoch:   495 | D_loss: -0.18859 | G_loss: 0.19522\n",
      "Epoch:   500 | D_loss: -0.15438 | G_loss: 0.11590\n",
      "Epoch:   505 | D_loss: -0.18884 | G_loss: 0.19340\n",
      "Epoch:   510 | D_loss: -0.18645 | G_loss: 0.19337\n",
      "Epoch:   515 | D_loss: -0.17965 | G_loss: 0.18805\n",
      "Epoch:   520 | D_loss: -0.18857 | G_loss: 0.19350\n",
      "Epoch:   525 | D_loss: -0.18939 | G_loss: 0.19425\n",
      "Epoch:   530 | D_loss: -0.18614 | G_loss: 0.19164\n",
      "Epoch:   535 | D_loss: -0.18554 | G_loss: 0.19023\n",
      "Epoch:   540 | D_loss: -0.18572 | G_loss: 0.19136\n",
      "Epoch:   545 | D_loss: -0.18548 | G_loss: 0.19004\n",
      "Epoch:   550 | D_loss: -0.18921 | G_loss: 0.19377\n",
      "Epoch:   555 | D_loss: -0.18394 | G_loss: 0.19234\n",
      "Epoch:   560 | D_loss: -0.08817 | G_loss: 0.13982\n",
      "Epoch:   565 | D_loss: -0.18956 | G_loss: 0.19366\n",
      "Epoch:   570 | D_loss: -0.18180 | G_loss: 0.19221\n",
      "Epoch:   575 | D_loss: -0.18707 | G_loss: 0.19222\n",
      "Epoch:   580 | D_loss: -0.18499 | G_loss: 0.19264\n",
      "Epoch:   585 | D_loss: -0.17888 | G_loss: 0.19037\n",
      "Epoch:   590 | D_loss: -0.18910 | G_loss: 0.19467\n",
      "Epoch:   595 | D_loss: -0.18780 | G_loss: 0.19453\n",
      "Epoch:   600 | D_loss: -0.18659 | G_loss: 0.19268\n",
      "Epoch:   605 | D_loss: -0.18614 | G_loss: 0.19198\n",
      "Epoch:   610 | D_loss: -0.18907 | G_loss: 0.19288\n",
      "Epoch:   615 | D_loss: -0.17521 | G_loss: 0.18025\n",
      "Epoch:   620 | D_loss: -0.02494 | G_loss: 0.01880\n",
      "Epoch:   625 | D_loss: -0.18395 | G_loss: 0.18929\n",
      "Epoch:   630 | D_loss: -0.18235 | G_loss: 0.19008\n",
      "Epoch:   635 | D_loss: -0.18707 | G_loss: 0.19355\n",
      "Epoch:   640 | D_loss: -0.18446 | G_loss: 0.19155\n",
      "Epoch:   645 | D_loss: -0.18841 | G_loss: 0.19284\n",
      "Epoch:   650 | D_loss: -0.18820 | G_loss: 0.19426\n",
      "Epoch:   655 | D_loss: -0.18674 | G_loss: 0.19368\n",
      "Epoch:   660 | D_loss: -0.18044 | G_loss: 0.18717\n",
      "Epoch:   665 | D_loss: -0.18770 | G_loss: 0.19230\n",
      "Epoch:   670 | D_loss: -0.03549 | G_loss: 0.19324\n",
      "Epoch:   675 | D_loss: -0.00371 | G_loss: -0.17248\n",
      "Epoch:   680 | D_loss: -0.18928 | G_loss: 0.19406\n",
      "Epoch:   685 | D_loss: -0.18629 | G_loss: 0.19213\n",
      "Epoch:   690 | D_loss: -0.03982 | G_loss: 0.18526\n",
      "Epoch:   695 | D_loss: -0.18483 | G_loss: 0.19273\n",
      "Epoch:   700 | D_loss: -0.18535 | G_loss: 0.19366\n",
      "Epoch:   705 | D_loss: -0.18543 | G_loss: 0.19126\n",
      "Epoch:   710 | D_loss: -0.18854 | G_loss: 0.19408\n",
      "Epoch:   715 | D_loss: -0.18453 | G_loss: 0.18966\n",
      "Epoch:   720 | D_loss: -0.18566 | G_loss: 0.19078\n",
      "Epoch:   725 | D_loss: -0.18029 | G_loss: 0.18432\n",
      "Epoch:   730 | D_loss: -0.17524 | G_loss: 0.19388\n",
      "Epoch:   735 | D_loss: -0.00639 | G_loss: 0.19573\n",
      "Epoch:   740 | D_loss: -0.18629 | G_loss: 0.19346\n",
      "Epoch:   745 | D_loss: -0.18359 | G_loss: 0.19066\n",
      "Epoch:   750 | D_loss: -0.18508 | G_loss: 0.19248\n",
      "Epoch:   755 | D_loss: -0.18513 | G_loss: 0.19032\n",
      "Epoch:   760 | D_loss: -0.18443 | G_loss: 0.19135\n",
      "Epoch:   765 | D_loss: -0.17568 | G_loss: 0.17576\n",
      "Epoch:   770 | D_loss: -0.18681 | G_loss: 0.19134\n",
      "Epoch:   775 | D_loss: -0.18607 | G_loss: 0.19183\n",
      "Epoch:   780 | D_loss: -0.18553 | G_loss: 0.19276\n",
      "Epoch:   785 | D_loss: -0.18223 | G_loss: 0.19390\n",
      "Epoch:   790 | D_loss: -0.10311 | G_loss: -0.02511\n",
      "Epoch:   795 | D_loss: -0.18619 | G_loss: 0.19354\n",
      "Epoch:   800 | D_loss: -0.18647 | G_loss: 0.19216\n",
      "Epoch:   805 | D_loss: -0.18606 | G_loss: 0.19222\n",
      "Epoch:   810 | D_loss: -0.18076 | G_loss: 0.18997\n",
      "Epoch:   815 | D_loss: -0.18506 | G_loss: 0.19140\n",
      "Epoch:   820 | D_loss: -0.18462 | G_loss: 0.19332\n",
      "Epoch:   825 | D_loss: -0.18101 | G_loss: 0.18963\n",
      "Epoch:   830 | D_loss: -0.18425 | G_loss: 0.19211\n",
      "Epoch:   835 | D_loss: -0.18398 | G_loss: 0.19078\n",
      "Epoch:   840 | D_loss: -0.18369 | G_loss: 0.19199\n",
      "Epoch:   845 | D_loss: -0.01328 | G_loss: 0.17561\n",
      "Epoch:   850 | D_loss: -0.18280 | G_loss: 0.19144\n",
      "Epoch:   855 | D_loss: -0.09020 | G_loss: -0.01588\n",
      "Epoch:   860 | D_loss: -0.18493 | G_loss: 0.18887\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "for epoch in range(max_epochs):\n",
    "    D.train()\n",
    "    G.train()\n",
    "    for i, (real_img, _) in enumerate(trainloader):\n",
    "        n_data = real_img.shape[0]\n",
    "        ## Real and Fake Images\n",
    "        real_img = real_img.to(device)\n",
    "        fake_img = G.generate(n_data, device)\n",
    "\n",
    "        ## Real and Fake Labels\n",
    "        real = -torch.ones((n_data, 1), device=device)\n",
    "        fake = torch.ones((n_data, 1), device=device)\n",
    "\n",
    "        ## Training Discriminator ##\n",
    "        d_optimizer.zero_grad()\n",
    "        # Real image -> Discriminator -> label Real\n",
    "        output = D(real_img)\n",
    "        d_real_loss = criterion(output, real)\n",
    "        \n",
    "        # Fake image -> Discriminator -> label Fake\n",
    "        output = D(fake_img.detach())\n",
    "        d_fake_loss = criterion(output, fake)\n",
    "        \n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Weight clipping\n",
    "        D.clip_weights()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            ## Training Generator ##\n",
    "            g_optimizer.zero_grad()\n",
    "            # Fake image -> Discriminator -> label Real\n",
    "            output = D(fake_img)\n",
    "            g_loss = criterion(output, real)        \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch: {epoch:5} | D_loss: {d_loss/2:.5f} | G_loss: {g_loss:.5f}\")\n",
    "\n",
    "    if epoch % 15 == 0:\n",
    "        G.eval()\n",
    "        epoch = str(epoch).zfill(4)\n",
    "        fake_img = G.generate(64, device)\n",
    "        save_image(fake_img, f\"output/WDCGAN/{epoch}.jpg\", nrow=8, normalize=True)\n",
    "        \n",
    "        torch.save(D, \"model/WDCGAN/critic.pth\")\n",
    "        torch.save(G, \"model/WDCGAN/generator.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 4 - Wasserstein DCGAN with PyTorch - CelebA Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
